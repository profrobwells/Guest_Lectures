#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(tfinal_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
trump_articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(tfinal_index)
View(trump_articles_df)
phrases <- c("Date:" = "",
"Article Text:" = "",
"Headline:" = "",
"Already have an account?" = "",
"Your information could be the missing piece to an important story. Submit your tip today and make a difference." = "",
"Stay up-to-date on the latestnews, podcasts, and more." = "",
"https" = "")
trump_articles_df <- trump_articles_df |>
# distinct(sentence, .keep_all = TRUE) |>
mutate(sentence = str_replace_all(sentence, phrases))
View(trump_articles_df)
write.csv(trump_articles_df, "trump_extracted_text_jan2025.csv")
#library(tidytext)
bigrams <- trump_articles_df %>%
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
bigrams
top_20_bigrams <- bigrams |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
top_20_bigrams
#library(tidytext)
trump_bigrams <- trump_articles_df %>%
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
bigrams
#library(tidytext)
harris_bigrams <- harris_articles_df %>%
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
harris_bigrams
top_20_harris_bigrams <- harris_bigrams |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
top_20_harris_bigrams
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
harris_sentiment <- harris_articles_df %>%
select(sentence, index) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
harris_sentiment_analysis <- sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index) %>%
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
harris_sentiment_analysis <- harris_sentiment_analysis %>%
group_by(index) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |>
inner_join(final_index, by=("index")) |>
select(index, date, sentiment, sentiment_type, filename, url)
top_20_trump_bigrams <- trump_bigrams |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
top_20_trump_bigrams
ggplot(top_20_trump_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
geom_bar(stat = "identity") +
theme(legend.position = "none") +
coord_flip() +
labs(title = "Top Two-Word phrases about Donald Trump on Daily Wire coverage",
caption = "n=99 articles, 2024. Graphic by Rob Wells. 1-23-2025",
x = "Phrase",
y = "Count of terms")
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
trump_sentiment <- trump_articles_df %>%
select(sentence, index) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
trump_sentiment_analysis <- trump_sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index) %>%
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
trump_sentiment_analysis <- trump_sentiment_analysis %>%
group_by(index) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |>
inner_join(final_index, by=("index")) |>
select(index, date, sentiment, sentiment_type, filename, url)
ggplot(trump_sentiment_analysis, aes(x = index, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge", stat = "identity") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
labs(title = "Sentiment of Daily Wire coverage of Donald Trump, October 2024",
caption = "n=99 articles. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Articles",
y = "Sentiment Score")
View(trump_sentiment)
View(trump_sentiment_analysis)
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
trump_sentiment <- trump_articles_df %>%
select(sentence, index) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
trump_sentiment_analysis <- trump_sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index) %>%
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
trump_sentiment_analysis <- trump_sentiment_analysis %>%
group_by(index) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |>
inner_join(tfinal_index, by=("index")) |>
select(index, date, sentiment, sentiment_type, filename, url)
ggplot(trump_sentiment_analysis, aes(x = index, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge", stat = "identity") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
labs(title = "Sentiment of Daily Wire coverage of Donald Trump, October 2024",
caption = "n=99 articles. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Articles",
y = "Sentiment Score")
View(tfinal_index)
View(trump_index)
harris_articles_df |>
mutate(candidate = harris)
harris_articles_df <- harris_articles_df |>
mutate(candidate = "harris")
View(harris_articles_df)
trump_articles_df <- trump_articles_df |>
mutate(candidate = "trump")
View(trump_articles_df)
combined <- rbind(harris_articles_df, trump_articles_df)
2911+3196
View(combined)
combined_sentiment <- combined %>%
select(sentence, index, candidate) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
View(combined_sentiment)
combined_sentiment <- combined %>%
select(sentence, index, candidate) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index, candidate) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
View(combined_sentiment)
combined_sentiment_analysis <- combined_sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index) %>%
summarize(sentiment = sum(value), .groups = "drop")
View(combined_sentiment)
View(combined_sentiment_analysis)
combined_sentiment_analysis <- combined_sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index, candidate) %>%
summarize(sentiment = sum(value), .groups = "drop")
View(combined_sentiment_analysis)
glimpse(combined)
combined_sentiment <- combined %>%
select(sentence, index, candidate, date) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index, candidate) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
combined_sentiment <- combined %>%
select(sentence, index, candidate, date) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index, candidate, date) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
View(combined_sentiment)
combined_sentiment <- combined %>%
select(sentence, candidate, date) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(date, candidate) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
View(combined_sentiment_analysis)
View(combined_sentiment)
combined_sentiment_analysis <- combined_sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(date, candidate) %>%
summarize(sentiment = sum(value), .groups = "drop")
combined_sentiment_analysis <- combined_sentiment_analysis %>%
group_by(date, candidate) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative"))
View(combined_sentiment)
View(combined_sentiment_analysis)
ggplot(combined_sentiment_analysis, aes(x = date, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge", stat = "identity") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
labs(title = "Comparing Sentiment of Daily Wire coverage of Harris and Trump, October 2024",
caption = "43 days. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Date",
y = "Sentiment Score")
View(combined_sentiment_analysis)
View(harris_articles_df)
Xharris_articles_df <- harris_articles_df |>
mutate(candidate = "harris") |>
mutate(date = case_when(
date =="2023-11-04" ~ "2024-11-04")
)
View(Xharris_articles_df)
Xharris_articles_df <- harris_articles_df |>
mutate(candidate = "harris") |>
mutate(date = case_when(
date =="2023-11-04" ~ "2024-11-04",
TRUE ~ date)
)
Xharris_articles_df <- harris_articles_df |>
mutate(candidate = "harris") |>
mutate(date = case_when(
str_detect(date, "2023-11-04") ~ "2024-11-04",
TRUE ~ date
))
glimpse(harris_articles_df)
Xharris_articles_df <- harris_articles_df |>
mutate(candidate = "harris") |>
mutate(date = case_when(
str_detect(as.character(date), "2023-11-04") ~ as.POSIXct("2024-11-04"),
TRUE ~ date
))
Xharris_articles_df <- harris_articles_df |>
mutate(candidate = "harris") |>
mutate(date = case_when(
str_detect(as.character(date), "2023-11-04") ~ as_datetime("2024-11-04"),
TRUE ~ date
))
harris_articles_df <- harris_articles_df |>
mutate(candidate = "harris") |>
mutate(date = case_when(
str_detect(as.character(date), "2023-11-04") ~ as_datetime("2024-11-04"),
TRUE ~ date
))
trump_articles_df <- trump_articles_df |>
mutate(candidate = "trump")
combined <- rbind(harris_articles_df, trump_articles_df)
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
combined_sentiment <- combined %>%
select(sentence, candidate, date) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(date, candidate) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
combined_sentiment_analysis <- combined_sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(date, candidate) %>%
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
combined_sentiment_analysis <- combined_sentiment_analysis %>%
group_by(date, candidate) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative"))
# |>
#   inner_join(tfinal_index, by=("index")) |>
#   select(index, date, sentiment, sentiment_type, filename, url)
ggplot(combined_sentiment_analysis, aes(x = date, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge", stat = "identity") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
labs(title = "Comparing Sentiment of Daily Wire coverage of Harris and Trump, October 2024",
caption = "43 days. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Date",
y = "Sentiment Score")
head(combined_sentiment_analysis)
ggplot(combined_sentiment_analysis,
aes(x = date, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
facet_wrap(~candidate, ncol = 1) +  # This creates separate panels for each candidate
labs(title = "Comparing Sentiment of Daily Wire coverage of Harris and Trump, October 2024",
caption = "43 days. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Date",
y = "Sentiment Score") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Makes date labels more readable
ggplot(combined_sentiment_analysis,
aes(x = date, y = sentiment, fill = candidate)) +
geom_col(position = "dodge") +
scale_fill_manual(values = c("harris" = "blue", "trump" = "red")) +
labs(title = "Comparing Sentiment of Daily Wire coverage of Harris and Trump, October 2024",
caption = "43 days. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Date",
y = "Sentiment Score") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
write.csv(combined, "trump-harris-DW-combined.csv")
library(tidyverse)
lat_index <- rio::import("./data/moley_lat.csv") |>
mutate(date = as.Date(entryDate, "%b %d, %Y")) |>
mutate(year2 = year(date))
lat_index %>%
mutate(years_match = year1 == year2) %>%
summarise(all_match = all(years_match))
lat_index %>%
filter(year1 != year2) %>%
select(year1, year2)
counts <- lat_index |>
count(year1)
sum(counts$n)
#2522 entries
full_index <- rio::import("./data/moley_lat_full_index.csv") |>
mutate(date = as.Date(PubDate, "%m/%d/%Y")) |>
mutate(year2 = year(date))
counts_full <- full_index |>
count(year2)
anti <- counts |>
anti_join(counts_full)
files_downloaded <- list.files("/Users/robwells/Library/CloudStorage/Dropbox/Current_Projects/Moley project 2024/Moley_LAT_column", pattern="*.pdf") %>%
as.data.frame() |>
rename(filename = 1) |>
mutate(filename1 = str_replace_all(filename, ".pdf", ""))
lat_index <- lat_index |>
mutate(filename1 = paste0(StoreId, "_1"))
updated_lat_moley_index <- lat_index |>
inner_join(files_downloaded, by=c("filename1")) |>
select(date, entryDate, year1, Title, Abstract, StoreId, filename1,filename)
anti <- files_downloaded |>
anti_join(lat_index, by=c("filename1"))
write.csv(updated_lat_moley_index, "updated_lat_moley_index.csv")
updated_lat_moley_index |>
count(date) |>
filter(n==2)
dupes <- updated_lat_moley_index |>
count(date, Title) |>
filter(n==2)
View(dupes)
View(lat_index)
View(updated_lat_moley_index)
#This tutorial shows how to copy files from one directory to another
# https://stackoverflow.com/questions/68995687/r-move-files-to-folder-based-on-list-or-column
inputdir  <- "./lat_sample"
targetdir <- "./renamed_lat"
df <- updated_lat_moley_index$Title
filestocopy <- list.files(inputdir, full.names = TRUE)
filestocopy <- unique(grep(paste(df,collapse="|"), filestocopy, value=TRUE))
install.packages("pdftools")
install.packages("tesseract")
install.packages("magick")
install.packages("pdftools")
# Define paths
pdf_folder <- "./lat_sample"
extracted_folder <- "./lat_scanned"
# Create extracted folder if it doesn't exist
if (!dir.exists(extracted_folder)) {
dir.create(extracted_folder)
}
# Loop through all files in the PDF folder
pdf_files <- list.files(pdf_folder, pattern = "\\.pdf$", full.names = TRUE)
for (pdf_file in pdf_files) {
output_file <- file.path(extracted_folder, paste0(tools::file_path_sans_ext(basename(pdf_file)), ".txt"))
pdf_text <- pdf_text(pdf_file)
writeLines(pdf_text, output_file)
cat("Text extracted from", pdf_file, "and saved to", output_file, "\n")
}
# Define paths
pdf_folder <- "./lat_sample"
extracted_folder <- "./lat_scanned"
# Create extracted folder if it doesn't exist
if (!dir.exists(extracted_folder)) {
dir.create(extracted_folder)
}
# Loop through all files in the PDF folder
pdf_files <- list.files(pdf_folder, pattern = "\\.png$", full.names = TRUE)
for (pdf_file in pdf_files) {
output_file <- file.path(extracted_folder, paste0(tools::file_path_sans_ext(basename(pdf_file)), ".txt"))
pdf_text <- pdf_text(pdf_file)
writeLines(pdf_text, output_file)
cat("Text extracted from", pdf_file, "and saved to", output_file, "\n")
}
# Define paths
pdf_folder <- "./lat_sample"
extracted_folder <- "./lat_scanned"
# Create extracted folder if it doesn't exist
if (!dir.exists(extracted_folder)) {
dir.create(extracted_folder)
}
# Loop through all files in the PDF folder
pdf_files <- list.files(pdf_folder, pattern = "\\.png$", full.names = TRUE)
for (file_path in pdf_files) {
# Construct the output path
output_filename <- paste0(tools::file_path_sans_ext(basename(file_path)), ".txt")
output_path <- file.path(extracted_folder, output_filename)
# Check the file extension and process accordingly
(grepl("\\.png$", file_path)) {
# Define paths
pdf_folder <- "./lat_sample"
extracted_folder <- "./lat_scanned"
# Create extracted folder if it doesn't exist
if (!dir.exists(extracted_folder)) {
dir.create(extracted_folder)
}
# Loop through all files in the PDF folder
pdf_files <- list.files(pdf_folder, pattern = "\\.png$", full.names = TRUE)
for (file_path in pdf_files) {
# Construct the output path
output_filename <- paste0(tools::file_path_sans_ext(basename(file_path)), ".txt")
output_path <- file.path(extracted_folder, output_filename)
# Check the file extension and process accordingly
grepl("\\.png$", file_path) {
# Define paths
pdf_folder <- "./lat_sample"
extracted_folder <- "./lat_scanned"
# Create extracted folder if it doesn't exist
if (!dir.exists(extracted_folder)) {
dir.create(extracted_folder)
}
# Loop through all files in the PDF folder
pdf_files <- list.files(pdf_folder, pattern = "\\.png$", full.names = TRUE)
for (file_path in pdf_files) {
# Construct the output path
output_filename <- paste0(tools::file_path_sans_ext(basename(file_path)), ".txt")
output_path <- file.path(extracted_folder, output_filename)
# Check the file extension and process accordingly
grepl("\\.png$", file_path)
# Extract text from PNG using tesseract
text <- ocr(file_path)
writeLines(text, output_path)
cat("Text extracted from", basename(file_path), "and saved to", output_filename, "\n")
}
library(pdftools)
library(tesseract)
library(magick)
library(pdftools)
library(tesseract)
library(magick)
# Define paths
pdf_folder <- "./lat_sample"
extracted_folder <- "./lat_scanned"
# Create extracted folder if it doesn't exist
if (!dir.exists(extracted_folder)) {
dir.create(extracted_folder)
}
# Loop through all files in the PDF folder
pdf_files <- list.files(pdf_folder, pattern = "\\.png$", full.names = TRUE)
for (file_path in pdf_files) {
# Construct the output path
output_filename <- paste0(tools::file_path_sans_ext(basename(file_path)), ".txt")
output_path <- file.path(extracted_folder, output_filename)
# Check the file extension and process accordingly
grepl("\\.png$", file_path)
# Extract text from PNG using tesseract
text <- ocr(file_path)
writeLines(text, output_path)
cat("Text extracted from", basename(file_path), "and saved to", output_filename, "\n")
}
