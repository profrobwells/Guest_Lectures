mutate(index = row_number())
files1 <- list.files("./kamala_extracted_text", pattern="*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
mutate(name = str_replace_all(filename, ".txt", "")) |>
mutate(name = str_replace_all(tolower(name), " ", "_")) |>
mutate(name = str_replace_all(name, "[[:punct:]]", "")) |>
arrange((name)) |>
mutate(index = row_number())
View(files1)
#load final data if you haven't already
final_data <- read.csv("assets/final_data.csv")
#load final data if you haven't already
final_data <- read.csv("/Users/robwells/Code/CompText_Jour/exercises/assets/final_data.csv")
View(final_data)
files <- files |>
inner_join(kharris_index, by=c("name")) |>
mutate(filepath = paste0("./kamala_extracted_text/", filename))
files <- list.files("./kamala_extracted_text", pattern="*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
mutate(name = str_replace_all(filename, ".txt", "")) |>
mutate(name = str_replace_all(tolower(name), " ", "_")) |>
mutate(name = str_replace_all(name, "[[:punct:]]", "")) |>
arrange((name)) |>
mutate(index = row_number())
final_index <- files |>
inner_join(kharris_index, by=c("name")) |>
mutate(filepath = paste0("./kamala_extracted_text/", filename))
###
# Define function to loop through each text file
###
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index %>%
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df %>%
bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(final_index)
#After viewing articles_df, I see 64 lines from the index that I don't need. Cutting them
articles_df <- articles_df %>%
slice(-c(1:64)) |>
#gets rid of blank rows
filter(trimws(sentence) != "")
write.csv(articles_df, "kharria_extracted_text_jan2025.csv")
View(articles_df)
articles_df |>
count(name)
articles_df |>
group_by(name) |>
count(name)
write.csv(articles_df, "kharris_extracted_text_jan2025.csv")
articles_df |>
group_by(name) |>
distinct() |>
count(name)
articles_df |>
group_by(name) |>
distinct(name) |>
count(name)
articles_df |>
group_by(name) |>
distinct(name) |>
count(name) |>
summarize(sum(n))
articles_df |>
group_by(name) |>
distinct(name) |>
count(name)
articles_df |>
group_by(name) |>
distinct(name) |>
count(name) |>
sum(n)
articles_df |>
group_by(name) |>
distinct(name) |>
count(name) |>
sum(n, na.rm = TRUE)
articles_df |>
group_by(name) |>
distinct(name) |>
count(name) |>
summarise(sum=n, na.rm = TRUE)
articles_df |>
distinct(name) |>
count(name) |>
summarise(sum=n, na.rm = TRUE)
articles_df |>
distinct(name) |>
count(name) |>
summarise(sum(n))
glimpse(final_index)
final_index <- final_data |>
inner_join(files1, c("index")) |>
#you need the actual hard-coded path on this line below to the text
mutate(filepath = paste0("./kamala_extracted_text/", filename)) |>
janitor::clean_names()
glimpse(final_index)
final_index <- files |>
inner_join(kharris_index, by=c("name")) |>
mutate(filepath = paste0("./kamala_extracted_text/", filename))
final_index <- files |>
inner_join(kharris_index, by=c("name")) |>
mutate(filepath = paste0("./kamala_extracted_text/", filename)) |>
janitor::clean_names()
glimpse(final_index)
final_index <- files |>
inner_join(kharris_index, by=c("name")) |>
mutate(filepath = paste0("./kamala_extracted_text/", filename)) |>
janitor::clean_names() |>
select(date,filename,url, filepath, filename)
###
# Define function to loop through each text file
###
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index %>%
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df %>%
bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(final_index)
#After viewing articles_df, I see 64 lines from the index that I don't need. Cutting them
articles_df <- articles_df %>%
slice(-c(1:64)) |>
#gets rid of blank rows
filter(trimws(sentence) != "")
write.csv(articles_df, "kharris_extracted_text_jan2025.csv")
articles_df |>
distinct(name) |>
count(name) |>
summarise(sum(n))
View(articles_df)
articles_df |>
distinct(filename) |>
count(filename) |>
summarise(sum(n))
articles_df |>
count(date)
x <- articles_df |>
distinct(sentence)
View(x)
x <- articles_df |>
distinct(sentence, .keep_all = TRUE)
View(x)
x |>
distinct(filename) |>
count(filename) |>
summarise(sum(n))
x <- articles_df |>
distinct(sentence, .keep_all = TRUE) |>
mutate(sentence = str_replace_all(sentence, phrases, ""))
phrases <- c("Date:" | "Article Text:" |"Headline:" |"Already have an account?" | "Your information could be the missing piece to an important story. Submit your tip today and make a difference." |
"Stay up-to-date on the latestnews, podcasts, and more.")
phrases <- c("Date:", "Article Text:","Headline:","Already have an account?", "Your information could be the missing piece to an important story. Submit your tip today and make a difference.",
"Stay up-to-date on the latestnews, podcasts, and more.")
x <- articles_df |>
distinct(sentence, .keep_all = TRUE) |>
mutate(sentence = str_replace_all(sentence, phrases, ""))
phrases <- c("Date:", "Article Text:","Headline:","Already have an account?", "Your information could be the missing piece to an important story. Submit your tip today and make a difference.",
"Stay up-to-date on the latestnews, podcasts, and more.")
x <- articles_df |>
distinct(sentence, .keep_all = TRUE) |>
mutate(sentence = str_replace_all(sentence, phrases))
phrases <- c("Date:" = "",
"Article Text:" = "",
"Headline:" = "",
"Already have an account?" = "",
"Your information could be the missing piece to an important story. Submit your tip today and make a difference." = "",
"Stay up-to-date on the latestnews, podcasts, and more." = "")
x <- articles_df |>
distinct(sentence, .keep_all = TRUE) |>
mutate(sentence = str_replace_all(sentence, phrases))
View(x)
x |>
distinct(filename) |>
count(filename) |>
summarise(sum(n))
phrases <- c("Date:" = "",
"Article Text:" = "",
"Headline:" = "",
"Already have an account?" = "",
"Your information could be the missing piece to an important story. Submit your tip today and make a difference." = "",
"Stay up-to-date on the latestnews, podcasts, and more." = "")
articles_df <- articles_df |>
distinct(sentence, .keep_all = TRUE) |>
mutate(sentence = str_replace_all(sentence, phrases))
write.csv(articles_df, "kharris_extracted_text_jan2025.csv")
#library(tidytext)
bigrams <- articles_df %>%
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
bigrams
top_20_bigrams <- bigrams |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
top_20_bigrams
#library(tidytext)
bigrams <- articles_df %>%
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
# filter(!word1 %in% stop_words$word) %>%
# filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
bigrams
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
geom_bar(stat = "identity") +
theme(legend.position = "none") +
coord_flip() +
labs(title = "Top Two-Word phrases about Kamala Harris on daily Wire coverage",
caption = "n=94 articles, 2024. Graphic by Rob Wells. 1-23-2025",
x = "Phrase",
y = "Count of terms")
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
sentiment <- articles_data %>%
select(sentence) %>%
mutate(article_nmbr = row_number()) |>
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(article_nmbr) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
sentiment <- articles_df %>%
select(sentence) %>%
mutate(article_nmbr = row_number()) |>
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(article_nmbr) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
sentiment_analysis <- sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(article_nmbr) %>%
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
sentiment_analysis <- sentiment_analysis %>%
group_by(article_nmbr) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative"))
View(sentiment_analysis)
final_index <- files |>
inner_join(kharris_index, by=c("name")) |>
mutate(filepath = paste0("./kamala_extracted_text/", filename)) |>
janitor::clean_names()
glimpse(final_index)
final_index <- files |>
inner_join(kharris_index, by=c("name")) |>
mutate(filepath = paste0("./kamala_extracted_text/", filename)) |>
janitor::clean_names() |>
rename(index = index_x) |>
select(date,filename,url, filepath, filename, index)
View(final_index)
final_index <- files |>
inner_join(kharris_index, by=c("name")) |>
mutate(filepath = paste0("./kamala_extracted_text/", filename)) |>
janitor::clean_names() |>
rename(index = index_x) |>
distinct(index, .keep_all = TRUE) |>
select(index, date,filename,url, filepath, filename)
anti <- files |>
anti_join(kharris_index, by=c("name"))
###
# Define function to loop through each text file
###
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index %>%
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df %>%
bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(final_index)
#After viewing articles_df, I see 64 lines from the index that I don't need. Cutting them
# articles_df <- articles_df %>%
#   slice(-c(1:64)) |>
#   #gets rid of blank rows
#     filter(trimws(sentence) != "")
phrases <- c("Date:" = "",
"Article Text:" = "",
"Headline:" = "",
"Already have an account?" = "",
"Your information could be the missing piece to an important story. Submit your tip today and make a difference." = "",
"Stay up-to-date on the latestnews, podcasts, and more." = "")
articles_df <- articles_df |>
# distinct(sentence, .keep_all = TRUE) |>
mutate(sentence = str_replace_all(sentence, phrases))
articles_df |>
distinct(filename) |>
count(filename) |>
summarise(sum(n))
View(articles_df)
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
sentiment <- articles_df %>%
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
sentiment <- articles_df %>%
select(sentence, index) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
sentiment_analysis <- sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index) %>%
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
sentiment_analysis <- sentiment_analysis %>%
group_by(index) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative"))
View(sentiment_analysis)
glimpse(articles_df)
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
sentiment <- articles_df %>%
select(sentence, index, filename) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
sentiment_analysis <- sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index) %>%
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
sentiment_analysis <- sentiment_analysis %>%
group_by(index) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative"))
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
sentiment <- articles_df %>%
select(sentence, index) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
sentiment_analysis <- sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index) %>%
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
sentiment_analysis <- sentiment_analysis %>%
group_by(index) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |>
inner_join(articles_df, by=("index"))
# aggregate at article level, total sentiment score (Positive or Negative)
sentiment_analysis <- sentiment_analysis %>%
group_by(index) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |>
inner_join(final_index, by=("index"))
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
sentiment <- articles_df %>%
select(sentence, index) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
group_by(index) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
sentiment_analysis <- sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index) %>%
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
sentiment_analysis <- sentiment_analysis %>%
group_by(index) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |>
inner_join(final_index, by=("index"))
View(sentiment_analysis)
sentiment_analysis <- sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index) %>%
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
sentiment_analysis <- sentiment_analysis %>%
group_by(index) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |>
inner_join(final_index, by=("index")) |>
select(index, date, sentiment, sentiment_type, filename, url)
