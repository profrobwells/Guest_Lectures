caption = "Oct 14-Nov 4, 2024. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Date",
y = "Sentiment Score") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Makes date labels more readable
View(combined)
headline_sentiment <- combined_index |>
select(filename, candidate, index) |>
mutate(text = str_replace_all(text, "-", " "))
headline_sentiment <- combined_index |>
select(filename, candidate, index) |>
mutate(text = str_replace_all(filename, "-", " "))
View(headline_sentiment)
headline_sentiment <- combined_index |>
select(filename, candidate, index) |>
mutate(text = str_replace_all(filename, "_", " "))
View(headline_sentiment)
headline_sentiment <- combined_index |>
select(filename, candidate, index) |>
mutate(text = str_replace_all(filename, "_", " "))
mutate(text = str_replace_all(text, ".txt", ""))
headline_sentiment <- combined_index |>
select(filename, candidate, index) |>
mutate(text = str_replace_all(filename, "_", " "))
mutate(text = str_replace_all(text, ".txt", ""))
headline_sentiment <- combined_index |>
select(filename, candidate, index) |>
mutate(text = str_replace_all(filename, "_", " "),
text = str_replace_all(text, ".txt", ""))
View(headline_sentiment)
headline_sentiment <- combined_index |>
select(filename, candidate, index) |>
mutate(text = str_replace_all(filename, "_", " "),
text = str_replace_all(text, ".txt", "")) |>    group_by(index, candidate) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
headline_sentiment_analysis <- headline_sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index, candidate) %>%
summarize(sentiment = sum(value), .groups = "drop") |>
inner_join(combined_index, by = c("index", "candidate"))
View(headline_sentiment_analysis)
headline_sentiment_analysis <- headline_sentiment_analysis%>%
group_by(index) %>%
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative"))
ggplot(headline_sentiment_analysis,
aes(x = date, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
facet_wrap(~candidate, ncol = 1) +  # This creates separate panels for each candidate
labs(title = "Harris, Trump Headline Sentiment in Daily Wire coverage",
caption = "Oct 14-Nov 4, 2024. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Date",
y = "Sentiment Score") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Makes date labels more readable
library(quanteda)
library(readtext)
# Create a corpus using the 'sentence' column as the text field
my_corpus <- corpus(combined, text_field = "sentence") # build a new corpus from the texts
head(my_corpus)
negative_bigrams <- combined_sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index, candidate) %>%
summarize(sentiment = sum(value), .groups = "drop") |>
inner_join(combined_index, by = c("index", "candidate"))
View(negative_bigrams)
negative_bigrams <- combined_sentiment %>%
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
View(combined_sentiment)
negative_bigrams <- combined_sentiment %>%
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index, candidate)
View(combined)
negative_bigrams <- combined
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
negative_bigrams <- combined |>
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
negative_bigrams <- combined |>
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) |>
inner_join(afinn, by = c("tokens"="word")) %>%
group_by(index, candidate)
negative_bigrams <- combined |>
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) |>
mutate(bigram = (word1, word2))
negative_bigrams <- combined |>
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) |>
mutate(bigram = (word1 & word2))
negative_bigrams <- combined |>
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
negative_bigrams <- combined |>
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) |>
mutate(bigram = paste0(word1, word2))
negative_bigrams <- combined |>
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) |>
mutate(bigram = paste0(word1," ", word2))
head(afinn)
negative_bigrams <- combined |>
select(sentence) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) |>
mutate(bigram = paste0(word1," ", word2))|>
left_join(afinn, by = c("word1" = "word")) %>%
rename(value1 = value) %>%
# Join for second word
left_join(afinn, by = c("word2" = "word")) %>%
rename(value2 = value) %>%
# Combine scores
mutate(
total_sentiment = coalesce(value1, 0) + coalesce(value2, 0)
)
negative_bigrams <- combined |>
select(sentence, candidate) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) |>
mutate(bigram = paste0(word1," ", word2))|>
left_join(afinn, by = c("word1" = "word")) %>%
rename(value1 = value) %>%
# Join for second word
left_join(afinn, by = c("word2" = "word")) %>%
rename(value2 = value) %>%
# Combine scores
mutate(
total_sentiment = coalesce(value1, 0) + coalesce(value2, 0)
)
negative_bigrams <- combined |>
select(sentence, candidate) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) |>
mutate(bigram = paste0(word1," ", word2))|>
left_join(afinn, by = c("word1" = "word")) %>%
rename(value1 = value) %>%
# Join for second word
left_join(afinn, by = c("word2" = "word")) %>%
rename(value2 = value) %>%
# Combine scores
mutate(
total_sentiment = coalesce(value1, 0) + coalesce(value2, 0)
) |>
filter(total_sentiment >= 0)
negative_bigrams <- combined |>
select(sentence, candidate) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) |>
mutate(bigram = paste0(word1," ", word2))|>
left_join(afinn, by = c("word1" = "word")) %>%
rename(value1 = value) %>%
# Join for second word
left_join(afinn, by = c("word2" = "word")) %>%
rename(value2 = value) %>%
# Combine scores
mutate(
total_sentiment = coalesce(value1, 0) + coalesce(value2, 0)
) |>
filter(total_sentiment < 0)
negative_bigrams <- combined |>
select(sentence, candidate) %>%
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(bigram, text, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(candidate, word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) |>
mutate(bigram = paste0(word1," ", word2))|>
left_join(afinn, by = c("word1" = "word")) %>%
rename(value1 = value) %>%
# Join for second word
left_join(afinn, by = c("word2" = "word")) %>%
rename(value2 = value) %>%
# Combine scores
mutate(
total_sentiment = coalesce(value1, 0) + coalesce(value2, 0)
) |>
filter(total_sentiment < 0)
top_50_negative_bigrams <- negative_bigrams |>
top_n(50) |>
select(bigram, total_sentiment, candidate) |>
arrange(desc(total_sentiment))
top_50_negative_bigrams
top_50_negative_bigrams <- negative_bigrams |>
top_n(50) |>
select(bigram, total_sentiment, candidate) |>
arrange(total_sentiment)
top_50_negative_bigrams
View(top_50_negative_bigrams)
top_50_negative_bigrams <- negative_bigrams |>
slice_min(total_sentiment, 50) |>
select(bigram, total_sentiment, candidate) |>
arrange(total_sentiment)
top_50_negative_bigrams <- negative_bigrams |>
slice_min(n= 50) |>
select(bigram, total_sentiment, candidate) |>
arrange(total_sentiment)
top_50_negative_bigrams <- negative_bigrams |>
slice_min(total_sentiment, n= 50) |>
select(bigram, total_sentiment, candidate) |>
arrange(total_sentiment)
top_50_negative_bigrams
top_50_negative_bigrams <- negative_bigrams |>
slice_min(total_sentiment, n= 50) |>
select(bigram, total_sentiment, candidate) |>
arrange(total_sentiment) |>
kable::kbl() %>%
kable_styling()
top_50_negative_bigrams <- negative_bigrams |>
slice_min(total_sentiment, n= 50) |>
select(bigram, total_sentiment, candidate) |>
arrange(total_sentiment) |>
kable::kbl() %>%
kable::kable_styling()
top_50_negative_bigrams <- negative_bigrams |>
slice_min(total_sentiment, n= 50) |>
select(bigram, total_sentiment, candidate) |>
arrange(total_sentiment)
knitr::kable(top_50_negative_bigrams), caption="Top Negative Phrases"))
top_50_negative_bigrams <- negative_bigrams |>
slice_min(total_sentiment, n= 50) |>
select(bigram, total_sentiment, candidate) |>
arrange(total_sentiment)
knitr::kable(top_50_negative_bigrams), caption="Top Negative Phrases")
top_50_negative_bigrams <- negative_bigrams |>
slice_min(total_sentiment, n= 50) |>
select(bigram, total_sentiment, candidate) |>
arrange(total_sentiment)
knitr::kable(top_50_negative_bigrams, caption = "Top Negative Phrases")
tibble::as_tibble(top_50_negative_bigrams)
print(tibble::as_tibble(top_50_negative_bigrams))
install.packages("DT")
library(DT)  # If you don't have it, install with: install.packages("DT")
datatable(top_50_negative_bigrams,
caption = "Top 50 Negative Phrases",
options = list(pageLength = 50))
top_20_harris_bigrams <- harris_bigrams |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
datatable(top_20_harris_bigrams,
caption = "Top 20 Harris Phrases",
options = list(pageLength = 20))
# Tokenize the corpus
my_tokens <- tokens(my_corpus)
# Perform KWIC (Key Word in Context) search on the tokens
quanteda_test <- kwic(my_tokens, pattern = "media", valuetype = "regex") |>
as.data.frame()
View(quanteda_test)
head(quanteda_test)
# Create a corpus using the 'sentence' column as the text field
my_corpus <- corpus(combined, text_field = "sentence") # build a new corpus from the texts
docvars(my_corpus)$candidate <- combined$candidate
head(my_corpus)
# Assuming your original data has a candidate field, when creating the corpus:
my_corpus <- corpus(combined, text_field = "text", docid_field = "filename")
# Assuming your original data has a candidate field, when creating the corpus:
my_corpus <- corpus(combined, text_field = "sentence", docid_field = "filename")
combined <- combined |>
mutate(doc_id = paste0(filename, "_", row_number()))
my_corpus <- corpus(combined, text_field = "sentence", docid_field = "doc_id")
docvars(my_corpus)$candidate <- combined$candidate
# Then proceed with your analysis
my_tokens <- tokens(my_corpus)
quanteda_test <- kwic(my_tokens, pattern = "media", valuetype = "regex") |>
as.data.frame() |>
left_join(docvars(my_corpus) |>
as.data.frame() |>
rownames_to_column("docname"),
by = "docname")
View(quanteda_test)
combined <- combined |>
mutate(doc_id = paste0(filename, "_", row_number()))
# Assuming your original data has a candidate field, when creating the corpus:
# Now create corpus with unique doc_id
my_corpus <- corpus(combined, text_field = "sentence", docid_field = "doc_id")
docvars(my_corpus)$candidate <- combined$candidate
# Then proceed with your analysis
my_tokens <- tokens(my_corpus)
quanteda_test <- kwic(my_tokens, pattern = "media", valuetype = "regex") |>
as.data.frame() |>
left_join(docvars(my_corpus) |>
as.data.frame() |>
rownames_to_column("docname"),
by = "docname")
head(quanteda_test)
# First, create a lookup dataframe from your original combined data
lookup_df <- combined |>
select(doc_id, candidate) |>
distinct()
# Now modify your KWIC analysis to join with this lookup
quanteda_test <- kwic(my_tokens, pattern = "media", valuetype = "regex") |>
as.data.frame() |>
# Extract the filename part before the underscore and row number
mutate(doc_id = docname) |>
left_join(lookup_df, by = "doc_id")
# Check the result
head(quanteda_test)
# Create a corpus using the 'sentence' column as the text field
my_corpus <- corpus(combined, text_field = "sentence") # build a new corpus from the texts
docvars(my_corpus)$candidate <- combined$candidate
head(my_corpus)
# First, create a lookup dataframe from your original combined data
lookup_df <- combined |>
select(doc_id, candidate) |>
distinct()
# Now modify your KWIC analysis to join with this lookup
quanteda_test <- kwic(my_tokens, pattern = "media", valuetype = "regex") |>
as.data.frame() |>
# Extract the filename part before the underscore and row number
mutate(doc_id = docname) |>
left_join(lookup_df, by = "doc_id")
# Check the result
head(quanteda_test)
quanteda_test2 <- kwic(my_tokens, phrase(c("media", "reporter", "newspaper", "journalist")), window = 50, valuetype = "regex") |> as.data.frame() |>
mutate(doc_id = docname) |>
left_join(lookup_df, by = "doc_id")
View(quanteda_test2)
quanteda_test2 <- kwic(my_tokens,
pattern = c("media", "reporter", "newspaper", "journalist"),
valuetype = "fixed") |>
as.data.frame() |>
mutate(doc_id = docname) |>
left_join(lookup_df, by = "doc_id")
View(quanteda_test2)
media_narratives <- kwic(my_tokens,
pattern = c("media", "reporter", "newspaper", "journalist"),
valuetype = "fixed") |>
as.data.frame() |>
mutate(doc_id = docname) |>
left_join(lookup_df, by = "doc_id")
media_narratives_table <- media_narratives |>
select(candidate, pre, keyword, post, doc_id)
datatable(media_narratives_table,
caption = "Top 50 Negative Phrases",
options = list(pageLength = 180))
media_narratives_table <- media_narratives |>
select(candidate, pre, keyword, post, doc_id)
datatable(media_narratives_table,
caption = "Top 50 Negative Phrases",
options = list(pageLength = 180))
combined |>
group_by(index, candidate) |>
count()
top_20_harris_bigrams <- harris_bigrams |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
top_20_harris_bigrams <- harris_bigrams |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
ggplot(harris_sentiment_analysis, aes(x = index, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge", stat = "identity") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
labs(title = "Sentiment of Daily Wire coverage of Kamala Harris, October 2024",
caption = "n=96 articles. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Articles",
y = "Sentiment Score")
ggplot(harris_sentiment_analysis, aes(x = index, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge", stat = "identity") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
labs(title = "Sentiment of Daily Wire coverage of Kamala Harris, October 2024",
caption = "n=96 articles. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Articles",
y = "Sentiment Score")
#Install packages if you don't have them already
#install.packages("tidyverse")
#install.packages("tidytext")
#install.packages("quanteda")
#install.packages("readtext")
#install.packages("DT")
library(tidyverse)
library(tidytext)
library(quanteda)
library(readtext)
library(DT)
files <- list.files("./kamala_extracted_text", pattern="*.txt") |>
as.data.frame() |>
rename(filename = 1) |>
mutate(name = str_replace_all(filename, ".txt", "")) |>
mutate(name = str_replace_all(tolower(name), " ", "_")) |>
mutate(name = str_replace_all(name, "[[:punct:]]", "")) |>
arrange((name)) |>
mutate(index = row_number())
#  #create an index with the file name
# mutate(index = str_extract(filename, "\\d+")) |>
#  mutate(index = as.numeric(index))
kharris_index <- rio::import("DW_Kamala_Articles.xlsx") |>
mutate(name = str_replace_all(tolower(NAME), " ", "_")) |>
mutate(name = str_replace_all(name, "[[:punct:]]", "")) |>
arrange((name)) |>
mutate(index = row_number()) |>
distinct(name, .keep_all = TRUE) |>
mutate(DATE = case_when(
str_detect(as.character(DATE), "2023-11-04") ~ as_datetime("2024-11-04"),
TRUE ~ DATE
))
View(kharris_index)
harris_articles_df <- read.csv("kharris_extracted_text_jan2025.csv")
harris_articles_df |>
distinct(filename) |>
count(filename) |>
summarise(sum(n))
